Model Comparison
    just approximations model
    While every model is wrong, some models will be worse than others
    how to compare two or more models used to explain the same data
    central problem in data analysis

Occam's razor
    two or more equivalent explanations for the same phenomenon, we should choose the simpler one
    
    how well the model fits the data (measures of accuracy)
        coefficient of determination R2
    tend to like high accuracy and those that are simple
        example : fit increasingly complex polynomials to a very simple dataset

    Too many parameters leads to overfitting
        in general, it is not a very good idea to use "polynomials" for real problems
        model with higher accuracy is not always what we really want
        model just "memorizing not learning" something from the data

        Gaussian "noise"

        Overfitting:
            when a model starts learning the "noise" in the data, 
                effectively hiding the interesting pattern
            more parameters ~ tendency to overfit the data

        But we must improve the accuracy by adding "more" parameters to the model
        use/ not used for fitting ~ within-sample accuracy vs out-of-sample accuracy

    Too few parameters leads to underfitting
        so simple that it is unable to capture the interesting pattern in the data

    The balance between "simplicity" and "accuracy"
        Things should be as simple as possible but not simpler
        model that neither overfits nor underfits the data
        trade-off and somehow we have to optimize or tune our models

        bias variance trade-off
            polyminal model can adapt to every single detail ~ high variance model
            a straight line ~ restricted model ~ more biased model

        High "bias": low ability to accommodate the data, miss the relevant pattern ~ underfitting
        High "variance": high sensitivity to data details, capture the noise in the data ~ overfitting

        increase one of these terms we decrease the other ~ bias-variance trade-off

Regularizing priors
    weakly informative priors
    two modifications on the least square method
        ridge regression
            using normal distributions for the beta coefficients
        Lasso regression
            using Laplace priors instead of Gaussian for the beta coefficients

    Laplace distribution ~ similar to the Gaussian distribution 
        first derivate is undefined at zero (very sharp peak at zero)
        probability mass much closer to zero
        tendency to make some parameters zero ~ removing some terms or variables from a model

    hierarchical models and shrinkage (fit a line to a single data point.) ~ hyper-priors
    
    Predictive accuracy measures
        simple or complex order ? how distinguish between those options

        2 methods to estimate the out-of-sample predictive accuracy:
            Cross-validation

            Information criteria
                The log-likelihood and the deviance
                Akaike information criterion            (AIC)
                Deviance information criterion          (DIC)
                Widely available information criterion  (WAIC)
                Pareto smoothed importance sampling leave-one-out cross-validation (LOOCV)
                Bayesian information criterion          (BIC)

Bayes factors
    Analogy with information criteria
    Computing Bayes factors
    
