1. Thinking Probabilistically - A Bayesian "Inference" Primer

    All about theoretical:
        "Statistical" modeling
        "Probabilities" and "uncertainty"
        Bayes' theorem and statistical inference
        "Single" parameter inference and the classic coin-flip problem
        Choosing "priors" and why people often "don't like" them, but "should"
        Communicating a Bayesian analysis

--------------------------------------------------------------------------
    "Statistics" as a form of modeling
        collecting, organizing, analyzing, and interpreting data
        make smt messy to clean and tidy

--------------------------------------------------------------------------
    Exploratory Data Analysis (EDA)
        sources: experiments, computer simulations, surveys, field observations, and so on
        • Descriptive statistics
            measures (or statistics)
                summarize or characterize the data in a "quantitative"
                using the mean, mode, standard deviation, interquartile ranges
        • Data visualization
            visually inspecting: histograms, scatter plots ...

--------------------------------------------------------------------------
    Inferential statistics
        data "generalization"
            understand the underlying "mechanism"
                how "generated" the data
                make "predictions"
        rely on probabilistic models
        Models:
            simplified "descriptions" of a given system (or process)
            capture only the most relevant aspects
        Bayesian modeling process:
            1. Given some data | "assumptions" | how it could have been generated
                "approximations" most of the time
            2. "conditioning" the model on our data
            3. check model "makes sense" in different criteria

--------------------------------------------------------------------------
    Probabilities and uncertainty
        - measure that quantifies the "uncertainty"
            + uncertainty is maximum if absence of information
            + world is "deterministic" or "stochastic"
                -> make probabilistic statements
                -> quantify uncertainty
        - binary "outcome"
        - Probabilities are numbers in the interval [0, 1]
        - product rule: p(A, B) = p(A| B) p(B)
        - "conditional" probability vs "unconditioned" probability
            + A and B are independent (unconditioned)
            + knowing B gives us useful information about A (conditional)
        - understanding Bayes' theorem need Conditional probabilities
            p( A| B) = p( A, B) / p(B)

--------------------------------------------------------------------------
    Probability "distributions"
        - mathematical object: describes how "likely" different events are (khả năng xảy ra)
            + restricted somehow to a set of "possible" events
            + think "data generated" from some probability distribution (PD) with "unobserved parameters"
            + Bayes' theorem to "invert" the relationship from data to parameters
        - building blocks of Bayesian models
            "combining" in proper ways to get useful "complex models"
        - famous PD: Gaussian | normal distribution
            + formula: mean | median and mode and standard deviation
            + a random variable:
                can't take any possible value
                values are strictly controlled by PD
                can't predict value but the "probability of value observing"
                COMMON Notation : "distributed as"
            + types of random variable:
                continuous: variables can take "any value" from some interval (float)
                discrete: variables take only "certain values" (int)
            + independent or dependency
                independent: iid variables | identically distributed
                dependency : non iid variables | temporal series
            + trends | seasonal |

--------------------------------------------------------------------------
    Bayes' theorem and statistical inference
            p(H|D) = p(D|H) * p(H) / p(D)
        + H: "hypothesis" or models
        + D: data
            probability of a hypothesis H given the data D
                • p(H): Prior
                    what we know about the value of some
                        parameter before seeing the data D.
                • p(D|H): Likelihood
                    how data introduce in our analysis
                • p(H|D): Posterior -> need calculate
                    the result of the Bayesian analysis
                    a probability distribution
                    balance of the prior and the likelihood
                    vague beliefs (niềm tin mơ hồ)
                    can be updated prior of a new analysis
                • p(D): Evidence
                    marginal likelihood
                    data observing probability averaged over all the possible values
                    care more about relative values vs absolute

--------------------------------------------------------------------------
    Single parameter inference : PAGE_13
        Probabilities are used to measure the uncertainty we have about parameters

        Bayes' theorem:
            + mechanism to correctly update probabilities when new data coming
            + hopefully reducing our uncertainty

        The coin-flipping problem
            • The general model
            • Choosing the likelihood
            • Choosing the prior
            • Getting the posterior
            • Computing and plotting the posterior

            Influence of the prior and how to choose one

        Communicating a Bayesian analysis
            Model notation and visualization
                communicate the model
                Kruschke's diagrams
                
            Summarizing the posterior
                Highest posterior density

        Posterior predictive checks
